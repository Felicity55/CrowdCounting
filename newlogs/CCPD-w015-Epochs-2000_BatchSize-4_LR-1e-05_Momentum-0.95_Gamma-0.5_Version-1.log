2022-12-27 16:38:52,746 - main - INFO - CCPD-w015-Epochs-2000_BatchSize-4_LR-1e-05_Momentum-0.95_Gamma-0.5_Version-1
2022-12-27 16:39:00,728 - main - INFO - Epoch 1/2000
2022-12-27 16:39:00,848 - main - INFO - ----------------------------
2022-12-27 16:39:01,006 - main - INFO - training...
2022-12-27 16:40:23,902 - main - INFO - CCPD-w015-Epochs-2000_BatchSize-4_LR-1e-05_Momentum-0.95_Gamma-0.5_Version-1
2022-12-27 16:40:28,052 - main - INFO - Epoch 1/2000
2022-12-27 16:40:28,052 - main - INFO - ----------------------------
2022-12-27 16:40:28,052 - main - INFO - training...
2022-12-27 16:45:18,847 - main - INFO - CCPD-w015-Epochs-2000_BatchSize-4_LR-1e-05_Momentum-0.95_Gamma-0.5_Version-1
2022-12-27 16:45:25,816 - main - INFO - Epoch 1/2000
2022-12-27 16:45:25,817 - main - INFO - ----------------------------
2022-12-27 16:45:25,817 - main - INFO - training...
2022-12-27 16:50:26,595 - main - DEBUG - Batch 500: running loss = 813450.9415
2022-12-27 16:55:25,031 - main - DEBUG - Batch 1000: running loss = 1392457.0508
2022-12-27 17:04:13,250 - main - INFO - CCPD-w015-Epochs-2000_BatchSize-4_LR-1e-05_Momentum-0.95_Gamma-0.5_Version-1
2022-12-27 17:04:17,862 - main - INFO - Epoch 1/2000
2022-12-27 17:04:17,862 - main - INFO - ----------------------------
2022-12-27 17:04:17,862 - main - INFO - training...
2022-12-27 17:09:20,202 - main - DEBUG - Batch 500: running loss = 587474.0060
2022-12-27 17:14:17,347 - main - DEBUG - Batch 1000: running loss = 1032925.7207
2022-12-27 17:19:14,555 - main - DEBUG - Batch 1500: running loss = 1390944.0961
2022-12-27 17:24:12,250 - main - DEBUG - Batch 2000: running loss = 1692578.0292
2022-12-27 17:29:10,109 - main - DEBUG - Batch 2500: running loss = 1954448.5987
2022-12-27 17:33:41,683 - main - INFO - Epoch 1 - train: epoch loss = 733.7749
2022-12-27 17:33:41,683 - main - INFO - validating...
2022-12-27 17:37:10,792 - main - DEBUG - Batch 500: running loss = 202538.7204
2022-12-27 17:50:47,638 - main - INFO - CCPD-w015-Epochs-2000_BatchSize-4_LR-1e-05_Momentum-0.95_Gamma-0.5_Version-1
2022-12-27 17:50:52,524 - main - INFO - Epoch 1/2000
2022-12-27 17:50:52,524 - main - INFO - ----------------------------
2022-12-27 17:50:52,524 - main - INFO - training...
2022-12-27 17:55:53,106 - main - DEBUG - Batch 500: running loss = 547359.5154
2022-12-27 18:00:55,219 - main - DEBUG - Batch 1000: running loss = 968726.1690
2022-12-27 18:05:57,169 - main - DEBUG - Batch 1500: running loss = 1311968.0778
2022-12-27 18:11:00,320 - main - DEBUG - Batch 2000: running loss = 1603927.7760
2022-12-27 18:16:01,352 - main - DEBUG - Batch 2500: running loss = 1860339.8183
2022-12-27 18:20:37,732 - main - INFO - Epoch 1 - train: epoch loss = 700.3684
2022-12-27 18:20:37,732 - main - INFO - validating...
2022-12-27 18:24:06,446 - main - DEBUG - Batch 500: running loss = 200721.0435
2022-12-27 18:27:32,843 - main - DEBUG - Batch 1000: running loss = 399576.0247
2022-12-27 18:45:32,598 - main - INFO - CCPD-w015-Epochs-2000_BatchSize-4_LR-1e-05_Momentum-0.95_Gamma-0.5_Version-1
2022-12-27 18:45:37,460 - main - INFO - Epoch 1/2000
2022-12-27 18:45:37,461 - main - INFO - ----------------------------
2022-12-27 18:45:37,461 - main - INFO - training...
2022-12-27 18:50:39,525 - main - DEBUG - Batch 500: running loss = 493291.7765
2022-12-27 18:55:41,037 - main - DEBUG - Batch 1000: running loss = 880877.3491
2022-12-27 19:00:44,018 - main - DEBUG - Batch 1500: running loss = 1201275.7227
2022-12-27 19:13:00,823 - main - DEBUG - Batch 2000: running loss = 1476930.1169
2022-12-27 19:31:04,491 - main - DEBUG - Batch 2500: running loss = 1719744.2772
2022-12-27 19:47:16,094 - main - INFO - Epoch 1 - train: epoch loss = 649.6468
2022-12-27 19:47:16,094 - main - INFO - validating...
2022-12-27 19:53:47,355 - main - DEBUG - Batch 500: running loss = 189405.8329
2022-12-27 20:00:19,916 - main - DEBUG - Batch 1000: running loss = 376555.7577
2022-12-27 20:00:30,899 - main - INFO - Epoch 1 - val: epoch loss = 376.4983
2022-12-27 20:00:31,083 - main - INFO - Saved the best model at epoch 1.
2022-12-27 20:00:31,083 - main - INFO - Epoch 2/2000
2022-12-27 20:00:31,099 - main - INFO - ----------------------------
2022-12-27 20:00:31,100 - main - INFO - training...
2022-12-27 20:17:19,452 - main - DEBUG - Batch 500: running loss = 202839.5175
2022-12-27 20:34:09,805 - main - DEBUG - Batch 1000: running loss = 392445.6981
2022-12-27 20:42:43,371 - main - DEBUG - Batch 1500: running loss = 569009.9289
2022-12-27 20:47:41,611 - main - DEBUG - Batch 2000: running loss = 738329.6794
2022-12-27 20:52:39,763 - main - DEBUG - Batch 2500: running loss = 900163.8828
2022-12-27 20:57:11,528 - main - INFO - Epoch 2 - train: epoch loss = 352.3654
2022-12-27 20:57:11,529 - main - INFO - validating...
2022-12-27 21:00:41,196 - main - DEBUG - Batch 500: running loss = 126471.5959
2022-12-27 21:04:10,117 - main - DEBUG - Batch 1000: running loss = 250942.0810
2022-12-27 21:04:15,966 - main - INFO - Epoch 2 - val: epoch loss = 250.8984
2022-12-27 21:04:16,131 - main - INFO - Saved the best model at epoch 2.
2022-12-27 21:04:16,131 - main - INFO - Epoch 3/2000
2022-12-27 21:04:16,132 - main - INFO - ----------------------------
2022-12-27 21:04:16,133 - main - INFO - training...
2022-12-27 21:09:13,812 - main - DEBUG - Batch 500: running loss = 150407.4189
2022-12-27 21:14:11,961 - main - DEBUG - Batch 1000: running loss = 295951.0248
2022-12-27 21:19:10,087 - main - DEBUG - Batch 1500: running loss = 436915.9174
2022-12-27 21:24:09,098 - main - DEBUG - Batch 2000: running loss = 575179.5237
2022-12-27 21:29:07,251 - main - DEBUG - Batch 2500: running loss = 710665.5744
2022-12-27 21:33:39,096 - main - INFO - Epoch 3 - train: epoch loss = 281.3148
2022-12-27 21:33:39,097 - main - INFO - validating...
2022-12-27 21:37:08,623 - main - DEBUG - Batch 500: running loss = 100844.1442
2022-12-27 21:40:37,436 - main - DEBUG - Batch 1000: running loss = 199569.2492
2022-12-27 21:40:43,277 - main - INFO - Epoch 3 - val: epoch loss = 199.5135
2022-12-27 21:40:43,435 - main - INFO - Saved the best model at epoch 3.
2022-12-27 21:40:43,435 - main - INFO - Epoch 4/2000
2022-12-27 21:40:43,436 - main - INFO - ----------------------------
2022-12-27 21:40:43,437 - main - INFO - training...
2022-12-27 21:45:41,005 - main - DEBUG - Batch 500: running loss = 131285.4178
2022-12-27 21:50:39,193 - main - DEBUG - Batch 1000: running loss = 260993.1701
2022-12-27 21:55:37,317 - main - DEBUG - Batch 1500: running loss = 388123.5454
2022-12-27 22:00:35,511 - main - DEBUG - Batch 2000: running loss = 511977.1144
2022-12-27 22:05:33,682 - main - DEBUG - Batch 2500: running loss = 634353.5851
2022-12-27 22:10:05,497 - main - INFO - Epoch 4 - train: epoch loss = 251.7195
2022-12-27 22:10:05,498 - main - INFO - validating...
2022-12-27 22:13:34,803 - main - DEBUG - Batch 500: running loss = 89276.6143
2022-12-27 22:17:03,471 - main - DEBUG - Batch 1000: running loss = 176775.9417
2022-12-27 22:17:09,319 - main - INFO - Epoch 4 - val: epoch loss = 176.7416
2022-12-27 22:17:09,482 - main - INFO - Saved the best model at epoch 4.
2022-12-27 22:17:09,483 - main - INFO - Epoch 5/2000
2022-12-27 22:17:09,483 - main - INFO - ----------------------------
2022-12-27 22:17:09,484 - main - INFO - training...
2022-12-27 22:22:07,097 - main - DEBUG - Batch 500: running loss = 120404.5786
2022-12-27 22:27:05,229 - main - DEBUG - Batch 1000: running loss = 241329.4211
2022-12-27 22:32:03,360 - main - DEBUG - Batch 1500: running loss = 358793.4812
2022-12-27 22:37:01,506 - main - DEBUG - Batch 2000: running loss = 476159.9451
2022-12-27 22:41:59,642 - main - DEBUG - Batch 2500: running loss = 592415.9689
2022-12-27 22:46:31,496 - main - INFO - Epoch 5 - train: epoch loss = 236.0158
2022-12-27 22:46:31,497 - main - INFO - validating...
2022-12-27 22:50:00,938 - main - DEBUG - Batch 500: running loss = 82778.1736
2022-12-27 22:53:29,872 - main - DEBUG - Batch 1000: running loss = 163590.8279
2022-12-27 22:53:35,693 - main - INFO - Epoch 5 - val: epoch loss = 163.5476
2022-12-27 22:53:35,853 - main - INFO - Saved the best model at epoch 5.
2022-12-27 22:53:35,854 - main - INFO - Epoch 6/2000
2022-12-27 22:53:35,854 - main - INFO - ----------------------------
2022-12-27 22:53:35,856 - main - INFO - training...
2022-12-27 22:58:33,377 - main - DEBUG - Batch 500: running loss = 114033.6922
2022-12-27 23:03:31,488 - main - DEBUG - Batch 1000: running loss = 228958.8621
2022-12-27 23:08:29,657 - main - DEBUG - Batch 1500: running loss = 342499.2359
2022-12-27 23:13:27,781 - main - DEBUG - Batch 2000: running loss = 456173.8544
2022-12-27 23:18:25,903 - main - DEBUG - Batch 2500: running loss = 567024.3815
2022-12-27 23:22:57,712 - main - INFO - Epoch 6 - train: epoch loss = 226.4944
2022-12-27 23:22:57,713 - main - INFO - validating...
2022-12-27 23:26:27,353 - main - DEBUG - Batch 500: running loss = 77772.6493
2022-12-27 23:29:56,433 - main - DEBUG - Batch 1000: running loss = nan
2022-12-27 23:30:02,299 - main - INFO - Epoch 6 - val: epoch loss = nan
2022-12-27 23:30:02,300 - main - INFO - Epoch 7/2000
2022-12-27 23:30:02,300 - main - INFO - ----------------------------
2022-12-27 23:30:02,301 - main - INFO - training...
2022-12-27 23:34:59,815 - main - DEBUG - Batch 500: running loss = 111031.3357
2022-12-27 23:39:57,931 - main - DEBUG - Batch 1000: running loss = 221638.5103
2022-12-27 23:44:56,040 - main - DEBUG - Batch 1500: running loss = 332012.0706
2022-12-27 23:49:54,128 - main - DEBUG - Batch 2000: running loss = 441345.1749
2022-12-27 23:54:52,203 - main - DEBUG - Batch 2500: running loss = 550435.3430
2022-12-27 23:59:24,258 - main - INFO - Epoch 7 - train: epoch loss = 219.9912
2022-12-27 23:59:24,259 - main - INFO - validating...
2022-12-28 00:02:53,871 - main - DEBUG - Batch 500: running loss = 74546.3834
2022-12-28 00:06:22,647 - main - DEBUG - Batch 1000: running loss = 147180.8951
2022-12-28 00:06:28,495 - main - INFO - Epoch 7 - val: epoch loss = 147.1375
2022-12-28 00:06:28,655 - main - INFO - Saved the best model at epoch 7.
2022-12-28 00:06:28,656 - main - INFO - Epoch 8/2000
2022-12-28 00:06:28,656 - main - INFO - ----------------------------
2022-12-28 00:06:28,657 - main - INFO - training...
2022-12-28 00:11:26,863 - main - DEBUG - Batch 500: running loss = 108979.0381
2022-12-28 00:16:25,770 - main - DEBUG - Batch 1000: running loss = 215885.5448
2022-12-28 00:21:24,760 - main - DEBUG - Batch 1500: running loss = 324736.0631
2022-12-28 00:26:23,744 - main - DEBUG - Batch 2000: running loss = 434578.0660
2022-12-28 00:31:22,802 - main - DEBUG - Batch 2500: running loss = 539853.8094
2022-12-28 00:35:55,455 - main - INFO - Epoch 8 - train: epoch loss = 215.2403
2022-12-28 00:35:55,456 - main - INFO - validating...
2022-12-28 00:39:25,309 - main - DEBUG - Batch 500: running loss = 72332.6646
2022-12-28 00:42:54,565 - main - DEBUG - Batch 1000: running loss = 142740.4392
2022-12-28 00:43:00,441 - main - INFO - Epoch 8 - val: epoch loss = 142.6950
2022-12-28 00:43:00,601 - main - INFO - Saved the best model at epoch 8.
2022-12-28 00:43:00,602 - main - INFO - Epoch 9/2000
2022-12-28 00:43:00,602 - main - INFO - ----------------------------
2022-12-28 00:43:00,603 - main - INFO - training...
2022-12-28 00:47:59,113 - main - DEBUG - Batch 500: running loss = 107189.6114
2022-12-28 00:52:58,204 - main - DEBUG - Batch 1000: running loss = 213052.8643
2022-12-28 00:57:57,391 - main - DEBUG - Batch 1500: running loss = 319084.1412
2022-12-28 01:02:56,527 - main - DEBUG - Batch 2000: running loss = 424745.2358
2022-12-28 01:07:55,717 - main - DEBUG - Batch 2500: running loss = 529575.0900
2022-12-28 01:12:28,452 - main - INFO - Epoch 9 - train: epoch loss = 211.5469
2022-12-28 01:12:28,453 - main - INFO - validating...
2022-12-28 01:15:58,184 - main - DEBUG - Batch 500: running loss = 70336.8182
2022-12-28 01:19:27,466 - main - DEBUG - Batch 1000: running loss = 138838.2897
2022-12-28 01:19:33,306 - main - INFO - Epoch 9 - val: epoch loss = 138.7984
2022-12-28 01:19:33,469 - main - INFO - Saved the best model at epoch 9.
2022-12-28 01:19:33,470 - main - INFO - Epoch 10/2000
2022-12-28 01:19:33,470 - main - INFO - ----------------------------
2022-12-28 01:19:33,471 - main - INFO - training...
2022-12-28 01:24:32,177 - main - DEBUG - Batch 500: running loss = 104562.4287
2022-12-28 01:29:31,442 - main - DEBUG - Batch 1000: running loss = 209381.6421
2022-12-28 01:34:30,746 - main - DEBUG - Batch 1500: running loss = 313670.0684
2022-12-28 01:39:29,859 - main - DEBUG - Batch 2000: running loss = 417501.7097
2022-12-28 01:44:28,362 - main - DEBUG - Batch 2500: running loss = 521216.9867
2022-12-28 01:49:00,542 - main - INFO - Epoch 10 - train: epoch loss = 208.5257
2022-12-28 01:49:00,542 - main - INFO - validating...
2022-12-28 01:52:31,122 - main - DEBUG - Batch 500: running loss = 68654.1881
2022-12-28 01:56:00,229 - main - DEBUG - Batch 1000: running loss = 135507.5836
2022-12-28 01:56:06,052 - main - INFO - Epoch 10 - val: epoch loss = 135.4661
2022-12-28 01:56:06,210 - main - INFO - Saved the best model at epoch 10.
2022-12-28 01:56:06,211 - main - INFO - Epoch 11/2000
2022-12-28 01:56:06,211 - main - INFO - ----------------------------
2022-12-28 01:56:06,212 - main - INFO - training...
2022-12-28 02:01:04,209 - main - DEBUG - Batch 500: running loss = 103328.8230
2022-12-28 02:06:02,755 - main - DEBUG - Batch 1000: running loss = 205709.0943
2022-12-28 02:11:01,344 - main - DEBUG - Batch 1500: running loss = 307764.6956
2022-12-28 02:15:59,930 - main - DEBUG - Batch 2000: running loss = 413459.6311
2022-12-28 02:20:58,483 - main - DEBUG - Batch 2500: running loss = 515190.1729
2022-12-28 02:25:30,679 - main - INFO - Epoch 11 - train: epoch loss = 205.8870
2022-12-28 02:25:30,679 - main - INFO - validating...
2022-12-28 02:29:01,710 - main - DEBUG - Batch 500: running loss = nan
2022-12-28 02:32:31,050 - main - DEBUG - Batch 1000: running loss = nan
2022-12-28 02:32:36,908 - main - INFO - Epoch 11 - val: epoch loss = nan
2022-12-28 02:32:36,908 - main - INFO - Epoch 12/2000
2022-12-28 02:32:36,908 - main - INFO - ----------------------------
2022-12-28 02:32:36,908 - main - INFO - training...
2022-12-28 02:37:34,896 - main - DEBUG - Batch 500: running loss = 102066.0458
2022-12-28 02:42:33,477 - main - DEBUG - Batch 1000: running loss = 204171.1180
2022-12-28 02:47:32,045 - main - DEBUG - Batch 1500: running loss = 303897.1687
2022-12-28 02:52:30,601 - main - DEBUG - Batch 2000: running loss = 405544.7500
2022-12-28 02:57:29,130 - main - DEBUG - Batch 2500: running loss = 508704.8694
2022-12-28 03:02:01,326 - main - INFO - Epoch 12 - train: epoch loss = 203.5684
2022-12-28 03:02:01,327 - main - INFO - validating...
2022-12-28 03:05:32,430 - main - DEBUG - Batch 500: running loss = nan
2022-12-28 03:09:01,711 - main - DEBUG - Batch 1000: running loss = nan
2022-12-28 03:09:07,554 - main - INFO - Epoch 12 - val: epoch loss = nan
2022-12-28 03:09:07,554 - main - INFO - Epoch 13/2000
2022-12-28 03:09:07,554 - main - INFO - ----------------------------
2022-12-28 03:09:07,554 - main - INFO - training...
2022-12-28 03:14:05,567 - main - DEBUG - Batch 500: running loss = 100979.1522
2022-12-28 03:19:04,163 - main - DEBUG - Batch 1000: running loss = 201056.8817
2022-12-28 03:24:02,749 - main - DEBUG - Batch 1500: running loss = 302436.8008
2022-12-28 03:29:01,334 - main - DEBUG - Batch 2000: running loss = 403446.5079
2022-12-28 03:33:59,934 - main - DEBUG - Batch 2500: running loss = 503780.5187
2022-12-28 03:38:32,148 - main - INFO - Epoch 13 - train: epoch loss = 201.2824
2022-12-28 03:38:32,148 - main - INFO - validating...
2022-12-28 03:42:04,034 - main - DEBUG - Batch 500: running loss = nan
2022-12-28 03:45:33,758 - main - DEBUG - Batch 1000: running loss = nan
2022-12-28 03:45:39,633 - main - INFO - Epoch 13 - val: epoch loss = nan
2022-12-28 03:45:39,633 - main - INFO - Epoch 14/2000
2022-12-28 03:45:39,633 - main - INFO - ----------------------------
2022-12-28 03:45:39,643 - main - INFO - training...
2022-12-28 03:50:37,678 - main - DEBUG - Batch 500: running loss = 99646.6272
2022-12-28 03:55:35,707 - main - DEBUG - Batch 1000: running loss = 199893.8162
2022-12-28 04:00:33,413 - main - DEBUG - Batch 1500: running loss = 299535.3648
2022-12-28 04:05:31,253 - main - DEBUG - Batch 2000: running loss = 398311.0619
2022-12-28 04:10:28,913 - main - DEBUG - Batch 2500: running loss = 497802.0102
2022-12-28 04:15:00,333 - main - INFO - Epoch 14 - train: epoch loss = 199.2617
2022-12-28 04:15:00,334 - main - INFO - validating...
2022-12-28 04:18:31,344 - main - DEBUG - Batch 500: running loss = nan
2022-12-28 04:22:00,164 - main - DEBUG - Batch 1000: running loss = nan
2022-12-28 04:22:06,054 - main - INFO - Epoch 14 - val: epoch loss = nan
2022-12-28 04:22:06,054 - main - INFO - Epoch 15/2000
2022-12-28 04:22:06,054 - main - INFO - ----------------------------
2022-12-28 04:22:06,055 - main - INFO - training...
2022-12-28 04:27:03,017 - main - DEBUG - Batch 500: running loss = 99994.7407
2022-12-28 04:32:00,338 - main - DEBUG - Batch 1000: running loss = 199299.8504
2022-12-28 04:36:57,660 - main - DEBUG - Batch 1500: running loss = 298240.9472
2022-12-28 04:41:54,989 - main - DEBUG - Batch 2000: running loss = 396764.7074
2022-12-28 04:46:52,340 - main - DEBUG - Batch 2500: running loss = 493285.7960
2022-12-28 04:51:23,359 - main - INFO - Epoch 15 - train: epoch loss = 197.1968
2022-12-28 04:51:23,359 - main - INFO - validating...
2022-12-28 04:54:52,449 - main - DEBUG - Batch 500: running loss = nan
2022-12-28 04:58:20,160 - main - DEBUG - Batch 1000: running loss = nan
2022-12-28 04:58:26,027 - main - INFO - Epoch 15 - val: epoch loss = nan
2022-12-28 04:58:26,027 - main - INFO - Epoch 16/2000
2022-12-28 04:58:26,027 - main - INFO - ----------------------------
2022-12-28 04:58:26,032 - main - INFO - training...
2022-12-28 05:03:22,900 - main - DEBUG - Batch 500: running loss = 97722.5542
2022-12-28 05:08:20,237 - main - DEBUG - Batch 1000: running loss = 194584.0889
2022-12-28 05:13:17,543 - main - DEBUG - Batch 1500: running loss = 292742.7437
2022-12-28 05:18:14,855 - main - DEBUG - Batch 2000: running loss = 391395.8512
2022-12-28 05:23:12,186 - main - DEBUG - Batch 2500: running loss = 488389.8390
2022-12-28 05:27:43,233 - main - INFO - Epoch 16 - train: epoch loss = 195.2184
2022-12-28 05:27:43,233 - main - INFO - validating...
2022-12-28 05:31:13,868 - main - DEBUG - Batch 500: running loss = 62453.9659
2022-12-28 05:34:42,551 - main - DEBUG - Batch 1000: running loss = nan
2022-12-28 05:34:48,384 - main - INFO - Epoch 16 - val: epoch loss = nan
2022-12-28 05:34:48,384 - main - INFO - Epoch 17/2000
2022-12-28 05:34:48,384 - main - INFO - ----------------------------
2022-12-28 05:34:48,384 - main - INFO - training...
2022-12-28 05:39:45,224 - main - DEBUG - Batch 500: running loss = 97577.3224
2022-12-28 05:44:42,577 - main - DEBUG - Batch 1000: running loss = 193435.3821
2022-12-28 05:49:39,949 - main - DEBUG - Batch 1500: running loss = 290903.0624
2022-12-28 05:54:37,266 - main - DEBUG - Batch 2000: running loss = 388354.1039
2022-12-28 05:59:34,567 - main - DEBUG - Batch 2500: running loss = 485033.7870
2022-12-28 06:04:05,647 - main - INFO - Epoch 17 - train: epoch loss = 193.1771
2022-12-28 06:04:05,647 - main - INFO - validating...
2022-12-28 06:07:35,888 - main - DEBUG - Batch 500: running loss = nan
2022-12-28 06:11:04,175 - main - DEBUG - Batch 1000: running loss = nan
2022-12-28 06:11:09,978 - main - INFO - Epoch 17 - val: epoch loss = nan
2022-12-28 06:11:09,978 - main - INFO - Epoch 18/2000
2022-12-28 06:11:09,978 - main - INFO - ----------------------------
2022-12-28 06:11:09,979 - main - INFO - training...
2022-12-28 06:16:06,764 - main - DEBUG - Batch 500: running loss = 95478.5918
2022-12-28 06:21:04,062 - main - DEBUG - Batch 1000: running loss = 189959.4646
2022-12-28 06:26:01,341 - main - DEBUG - Batch 1500: running loss = 285940.9542
2022-12-28 06:30:58,638 - main - DEBUG - Batch 2000: running loss = 382090.6035
2022-12-28 06:35:55,875 - main - DEBUG - Batch 2500: running loss = 477818.9930
2022-12-28 06:40:26,905 - main - INFO - Epoch 18 - train: epoch loss = 191.2245
2022-12-28 06:40:26,905 - main - INFO - validating...
2022-12-28 06:43:57,469 - main - DEBUG - Batch 500: running loss = nan
2022-12-28 06:47:26,276 - main - DEBUG - Batch 1000: running loss = nan
2022-12-28 06:47:32,116 - main - INFO - Epoch 18 - val: epoch loss = nan
2022-12-28 06:47:32,116 - main - INFO - Epoch 19/2000
2022-12-28 06:47:32,116 - main - INFO - ----------------------------
2022-12-28 06:47:32,116 - main - INFO - training...
2022-12-28 06:52:28,838 - main - DEBUG - Batch 500: running loss = 93760.7706
2022-12-28 06:57:26,156 - main - DEBUG - Batch 1000: running loss = 188375.2221
2022-12-28 07:02:23,406 - main - DEBUG - Batch 1500: running loss = 283127.7677
2022-12-28 07:07:20,686 - main - DEBUG - Batch 2000: running loss = 378966.7374
2022-12-28 07:12:17,932 - main - DEBUG - Batch 2500: running loss = 474069.9609
2022-12-28 07:16:48,896 - main - INFO - Epoch 19 - train: epoch loss = 189.1769
2022-12-28 07:16:48,896 - main - INFO - validating...
2022-12-28 07:20:19,419 - main - DEBUG - Batch 500: running loss = nan
2022-12-28 07:23:48,233 - main - DEBUG - Batch 1000: running loss = nan
2022-12-28 07:23:54,073 - main - INFO - Epoch 19 - val: epoch loss = nan
2022-12-28 07:23:54,073 - main - INFO - Epoch 20/2000
2022-12-28 07:23:54,073 - main - INFO - ----------------------------
2022-12-28 07:23:54,083 - main - INFO - training...
2022-12-28 07:28:50,803 - main - DEBUG - Batch 500: running loss = 95032.6392
2022-12-28 07:33:48,133 - main - DEBUG - Batch 1000: running loss = 188699.1071
2022-12-28 07:38:45,479 - main - DEBUG - Batch 1500: running loss = 282210.8631
2022-12-28 07:43:42,848 - main - DEBUG - Batch 2000: running loss = 375487.0852
2022-12-28 07:48:40,355 - main - DEBUG - Batch 2500: running loss = 468195.6440
2022-12-28 07:53:11,858 - main - INFO - Epoch 20 - train: epoch loss = 187.1685
2022-12-28 07:53:11,859 - main - INFO - validating...
2022-12-28 07:56:42,433 - main - DEBUG - Batch 500: running loss = nan
2022-12-28 08:00:10,906 - main - DEBUG - Batch 1000: running loss = nan
2022-12-28 08:00:16,713 - main - INFO - Epoch 20 - val: epoch loss = nan
2022-12-28 08:00:16,713 - main - INFO - Epoch 21/2000
2022-12-28 08:00:16,713 - main - INFO - ----------------------------
2022-12-28 08:00:16,713 - main - INFO - training...
2022-12-28 08:05:14,126 - main - DEBUG - Batch 500: running loss = 94154.9352
2022-12-28 08:10:12,132 - main - DEBUG - Batch 1000: running loss = 187488.8553
2022-12-28 08:15:10,186 - main - DEBUG - Batch 1500: running loss = 280173.6366
2022-12-28 08:20:08,248 - main - DEBUG - Batch 2000: running loss = 372292.5318
2022-12-28 08:25:06,388 - main - DEBUG - Batch 2500: running loss = 462664.4492
2022-12-28 08:29:38,174 - main - INFO - Epoch 21 - train: epoch loss = 185.2036
2022-12-28 08:29:38,174 - main - INFO - validating...
2022-12-28 08:33:07,852 - main - DEBUG - Batch 500: running loss = 58893.6087
2022-12-28 08:36:36,079 - main - DEBUG - Batch 1000: running loss = nan
2022-12-28 08:36:41,880 - main - INFO - Epoch 21 - val: epoch loss = nan
2022-12-28 08:36:41,885 - main - INFO - Epoch 22/2000
2022-12-28 08:36:41,885 - main - INFO - ----------------------------
2022-12-28 08:36:41,886 - main - INFO - training...
2022-12-28 08:41:39,525 - main - DEBUG - Batch 500: running loss = 92214.3951
2022-12-28 08:46:37,755 - main - DEBUG - Batch 1000: running loss = 183701.8806
2022-12-28 08:51:36,017 - main - DEBUG - Batch 1500: running loss = 274061.3199
2022-12-28 08:56:34,313 - main - DEBUG - Batch 2000: running loss = 367711.4828
2022-12-28 09:01:32,601 - main - DEBUG - Batch 2500: running loss = 458215.6722
2022-12-28 09:06:04,813 - main - INFO - Epoch 22 - train: epoch loss = 183.2219
2022-12-28 09:06:04,813 - main - INFO - validating...
2022-12-28 09:09:35,720 - main - DEBUG - Batch 500: running loss = nan
2022-12-28 09:13:04,534 - main - DEBUG - Batch 1000: running loss = nan
2022-12-28 09:13:10,400 - main - INFO - Epoch 22 - val: epoch loss = nan
2022-12-28 09:13:10,407 - main - INFO - Epoch 23/2000
2022-12-28 09:13:10,407 - main - INFO - ----------------------------
2022-12-28 09:13:10,408 - main - INFO - training...
2022-12-28 09:18:08,153 - main - DEBUG - Batch 500: running loss = 91757.5286
2022-12-28 09:23:06,499 - main - DEBUG - Batch 1000: running loss = 182495.0688
2022-12-28 09:28:04,905 - main - DEBUG - Batch 1500: running loss = 274767.0665
2022-12-28 09:33:03,328 - main - DEBUG - Batch 2000: running loss = 364283.3452
2022-12-28 09:38:01,714 - main - DEBUG - Batch 2500: running loss = 454716.5680
2022-12-28 09:42:33,784 - main - INFO - Epoch 23 - train: epoch loss = 181.1998
2022-12-28 09:42:33,784 - main - INFO - validating...
2022-12-28 09:46:04,447 - main - DEBUG - Batch 500: running loss = nan
2022-12-28 09:49:33,311 - main - DEBUG - Batch 1000: running loss = nan
2022-12-28 09:49:39,155 - main - INFO - Epoch 23 - val: epoch loss = nan
2022-12-28 09:49:39,155 - main - INFO - Epoch 24/2000
2022-12-28 09:49:39,155 - main - INFO - ----------------------------
2022-12-28 09:49:39,161 - main - INFO - training...
2022-12-28 09:54:37,051 - main - DEBUG - Batch 500: running loss = 90301.1275
2022-12-28 09:59:35,797 - main - DEBUG - Batch 1000: running loss = 179267.7677
2022-12-28 10:04:34,716 - main - DEBUG - Batch 1500: running loss = 270178.1060
2022-12-28 10:09:33,623 - main - DEBUG - Batch 2000: running loss = 359565.2344
2022-12-28 10:14:32,521 - main - DEBUG - Batch 2500: running loss = 447968.6849
2022-12-28 10:19:04,998 - main - INFO - Epoch 24 - train: epoch loss = 179.3462
2022-12-28 10:19:04,998 - main - INFO - validating...
2022-12-28 10:22:35,945 - main - DEBUG - Batch 500: running loss = nan
2022-12-28 10:26:04,711 - main - DEBUG - Batch 1000: running loss = nan
2022-12-28 10:26:10,526 - main - INFO - Epoch 24 - val: epoch loss = nan
2022-12-28 10:26:10,527 - main - INFO - Epoch 25/2000
2022-12-28 10:26:10,527 - main - INFO - ----------------------------
2022-12-28 10:26:10,528 - main - INFO - training...
2022-12-28 10:31:08,379 - main - DEBUG - Batch 500: running loss = 89644.6451
2022-12-28 10:36:07,291 - main - DEBUG - Batch 1000: running loss = 178687.3049
2022-12-28 10:41:06,232 - main - DEBUG - Batch 1500: running loss = 267895.3358
2022-12-28 10:46:05,166 - main - DEBUG - Batch 2000: running loss = 355846.2512
2022-12-28 10:51:04,106 - main - DEBUG - Batch 2500: running loss = 444744.6656
2022-12-28 10:55:36,676 - main - INFO - Epoch 25 - train: epoch loss = 177.2309
2022-12-28 10:55:36,676 - main - INFO - validating...
2022-12-28 10:59:06,299 - main - DEBUG - Batch 500: running loss = nan
2022-12-28 11:02:34,563 - main - DEBUG - Batch 1000: running loss = nan
2022-12-28 11:02:40,396 - main - INFO - Epoch 25 - val: epoch loss = nan
2022-12-28 11:02:40,396 - main - INFO - Epoch 26/2000
2022-12-28 11:02:40,396 - main - INFO - ----------------------------
2022-12-28 11:02:40,396 - main - INFO - training...
2022-12-28 11:07:38,666 - main - DEBUG - Batch 500: running loss = 88047.9396
2022-12-28 11:12:38,079 - main - DEBUG - Batch 1000: running loss = 177602.0744
2022-12-28 11:17:38,508 - main - DEBUG - Batch 1500: running loss = 265315.2875
2022-12-28 11:22:58,937 - main - DEBUG - Batch 2000: running loss = 352923.4513
2022-12-28 11:27:13,934 - main - INFO - Saved the best model at epoch 26.
